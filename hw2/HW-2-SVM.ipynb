{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1335b064",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bag_of_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 186\u001b[0m\n\u001b[1;32m    184\u001b[0m y_train \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m review \u001b[38;5;129;01min\u001b[39;00m train_data:\n\u001b[0;32m--> 186\u001b[0m     X_train\u001b[38;5;241m.\u001b[39mappend(bag_of_words(review[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))  \u001b[38;5;66;03m# Convert review text to bag-of-words\u001b[39;00m\n\u001b[1;32m    187\u001b[0m     y_train\u001b[38;5;241m.\u001b[39mappend(review[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])  \n\u001b[1;32m    190\u001b[0m X_val \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bag_of_words' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import Counter\n",
    "import time\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def folder_list(path,label):\n",
    "    '''\n",
    "    PARAMETER PATH IS THE PATH OF YOUR LOCAL FOLDER\n",
    "    '''\n",
    "    filelist = os.listdir(path)\n",
    "    review = []\n",
    "    for infile in filelist:\n",
    "        file = os.path.join(path,infile)\n",
    "        r = read_data(file)\n",
    "        r.append(label)\n",
    "        review.append(r)\n",
    "    return review\n",
    "\n",
    "def read_data(file):\n",
    "    '''\n",
    "    Read each file into a list of strings.\n",
    "    Example:\n",
    "    [\"it's\", 'a', 'curious', 'thing', \"i've\", 'found', 'that', 'when', 'willis', 'is', 'not', 'called', 'on',\n",
    "    ...'to', 'carry', 'the', 'whole', 'movie', \"he's\", 'much', 'better', 'and', 'so', 'is', 'the', 'movie']\n",
    "    '''\n",
    "    f = open(file)\n",
    "    lines = f.read().split(' ')\n",
    "    symbols = '${}()[].,:;+-*/&|<>=~\" '\n",
    "    words = map(lambda Element: Element.translate(str.maketrans(\"\", \"\", symbols)).strip(), lines)\n",
    "    words = filter(None, words)\n",
    "    return list(words)\n",
    "\n",
    "\n",
    "def load_and_shuffle_data():\n",
    "    '''\n",
    "    pos_path is where you save positive review data.\n",
    "    neg_path is where you save negative review data.\n",
    "    '''\n",
    "    pos_path = \"/Users/tapankhaladkar/Machine Learning/hw2/data/pos\"\n",
    "    neg_path = \"/Users/tapankhaladkar/Machine Learning/hw2/data/neg\"\n",
    "\n",
    "    pos_review = folder_list(pos_path,1)\n",
    "    neg_review = folder_list(neg_path,-1)\n",
    "\n",
    "    review = pos_review + neg_review\n",
    "    random.shuffle(review)\n",
    "    return review\n",
    "\n",
    "# Taken from http://web.stanford.edu/class/cs221/ Assignment #2 Support Code\n",
    "def dotProduct(d1, d2):\n",
    "    \"\"\"\n",
    "    @param dict d1: a feature vector represented by a mapping from a feature (string) to a weight (float).\n",
    "    @param dict d2: same as d1\n",
    "    @return float: the dot product between d1 and d2\n",
    "    \"\"\"\n",
    "    if len(d1) < len(d2):\n",
    "        return dotProduct(d2, d1)\n",
    "    else:\n",
    "        return sum(d1.get(f, 0) * v for f, v in d2.items())\n",
    "\n",
    "def increment(d1, scale, d2):\n",
    "    \"\"\"\n",
    "    Implements d1 += scale * d2 for sparse vectors.\n",
    "    @param dict d1: the feature vector which is mutated.\n",
    "    @param float scale\n",
    "    @param dict d2: a feature vector.\n",
    "\n",
    "    NOTE: This function does not return anything, but rather\n",
    "    increments d1 in place. We do this because it is much faster to\n",
    "    change elements of d1 in place than to build a new dictionary and\n",
    "    return it.\n",
    "    \"\"\"\n",
    "    for f, v in d2.items():\n",
    "        d1[f] = d1.get(f, 0) + v * scale\n",
    "        \n",
    "        \n",
    "def bow(words):\n",
    "    return dict(Counter(words))\n",
    "\n",
    "\n",
    "\n",
    "def pegasos_sparse(X, y, lambda_param, max_epochs=1000, tolerance=0.001):\n",
    "    n = len(X)\n",
    "    w = {}  \n",
    "    t = 0  \n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        misclassified = 0\n",
    "        w_previous = copy.deepcopy(w) \n",
    "        \n",
    "        data = list(zip(X, y))\n",
    "        random.shuffle(data)\n",
    "        \n",
    "        for x_i, y_i in data:\n",
    "            t += 1\n",
    "            eta = 1 / (lambda_param * t)\n",
    "            \n",
    "            if dotProduct(w, x_i) * y_i < 1:\n",
    "                increment(w, -lambda_param * eta, w)  # w = (1 - eta * lambda) * w\n",
    "                increment(w, eta * y_i, x_i)  # w += eta * y_i * x_i\n",
    "                misclassified += 1\n",
    "            else:\n",
    "                increment(w, -lambda_param * eta, w)  # w = (1 - eta * lambda) * w\n",
    "        \n",
    "        error_rate = misclassified / n\n",
    "        if error_rate < tolerance:\n",
    "            print(f\"Converged after {epoch + 1} epochs (Error rate: {error_rate:.6f})\")\n",
    "            break\n",
    "        \n",
    "        weight_change = sum((w.get(k, 0) - w_previous.get(k, 0))**2 for k in set(w) | set(w_previous))\n",
    "        if weight_change < tolerance:\n",
    "            print(f\"Converged after {epoch + 1} epochs (Weight change: {weight_change:.6f})\")\n",
    "            break\n",
    "    \n",
    "    return w\n",
    "\n",
    "def pegasos_sparse_optimized(X, y, lambda_param, max_epochs=1000, tolerance=0.001):\n",
    "    n = len(X)\n",
    "    s = 1  \n",
    "    W = {}  \n",
    "    t = 2  \n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        misclassified = 0\n",
    "        \n",
    "        data = list(zip(X, y))\n",
    "        random.shuffle(data)\n",
    "        \n",
    "        for x_i, y_i in data:\n",
    "            eta = 1 / (lambda_param * t)\n",
    "            \n",
    "            s = s * (1 - eta * lambda_param)\n",
    "            \n",
    "            if s == 0:\n",
    "                s = 1\n",
    "                W.clear()  \n",
    "            else:\n",
    "                if y_i * dotProduct(W, x_i) < 1/s:\n",
    "                    increment(W, (eta * y_i) / s, x_i)\n",
    "                    misclassified += 1\n",
    "            \n",
    "            t += 1\n",
    "        \n",
    "        error_rate = misclassified / n\n",
    "        if error_rate < tolerance:\n",
    "            print(f\"Converged after {epoch + 1} epochs (Error rate: {error_rate:.6f})\")\n",
    "            break\n",
    "    \n",
    "    return s, W\n",
    "\n",
    "\n",
    "def compute_norm(w):\n",
    "    return sum(v**2 for v in w.values())**0.5\n",
    "\n",
    "def predict_original(w, x):\n",
    "    return 1 if dotProduct(w, x) >= 0 else -1\n",
    "\n",
    "def predict_optimized(s, W, x):\n",
    "    return 1 if dotProduct(W, x) >= 0 else -1\n",
    "\n",
    "\n",
    "def classification_error(w, X, y):\n",
    "    errors = sum(1 for x_i, y_i in zip(X, y) if y_i * dotProduct(w, x_i) < 0)\n",
    "    return errors / len(y)\n",
    "\n",
    "def search_lambda(X_train, y_train, X_val, y_val, lambda_range):\n",
    "    results = []\n",
    "    for lambda_param in lambda_range:\n",
    "        s, W = pegasos_sparse_optimized(X_train, y_train, lambda_param)\n",
    "        val_error = classification_error({k: v*s for k, v in W.items()}, X_val, y_val)\n",
    "        results.append((lambda_param, val_error))\n",
    "        print(f\"Î» = {lambda_param:.6f}, Validation Error = {val_error:.4f}\")\n",
    "    return results\n",
    "\n",
    "\n",
    "all_data = load_and_shuffle_data()\n",
    "train_data = all_data[:1500]\n",
    "val_data = all_data[1500:]\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "for review in train_data:\n",
    "    X_train.append(bag_of_words(review[:-1]))  # Convert review text to bag-of-words\n",
    "    y_train.append(review[-1])  \n",
    "\n",
    "\n",
    "X_val = []\n",
    "y_val = []\n",
    "for review in val_data:\n",
    "    X_val.append(bag_of_words(review[:-1]))  \n",
    "    y_val.append(review[-1])  \n",
    "\n",
    "# Print some statistics\n",
    "print(f\"Number of training examples: {len(X_train)}\")\n",
    "print(f\"Number of validation examples: {len(X_val)}\")\n",
    "print(f\"Sample training example:\\n{list(X_train[0].items())[:5]}...\")\n",
    "print(f\"Corresponding label: {y_train[0]}\")\n",
    "\n",
    "max_epochs = 2\n",
    "lambda_param = 0.1\n",
    "\n",
    "start_time = time.time()\n",
    "w = pegasos_sparse(X_train, y_train, lambda_param, max_epochs)\n",
    "original_time = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "s, W = pegasos_sparse_optimized(X_train, y_train, lambda_param, max_epochs)\n",
    "optimized_time = time.time() - start_time\n",
    "\n",
    "# Compare results\n",
    "norm_original = compute_norm(w)\n",
    "norm_optimized = s * compute_norm(W)\n",
    "\n",
    "print(f\"Original Pegasos runtime: {original_time:.4f} seconds\")\n",
    "print(f\"Optimized Pegasos runtime: {optimized_time:.4f} seconds\")\n",
    "print(f\"Norm of original weight vector: {norm_original:.6f}\")\n",
    "print(f\"Norm of optimized weight vector: {norm_optimized:.6f}\")\n",
    "print(f\"Relative difference in norms: {abs(norm_original - norm_optimized) / norm_original:.6f}\")\n",
    "\n",
    "# Check predictions\n",
    "num_checks = 100\n",
    "mismatches = 0\n",
    "for x in X_train[:num_checks]:\n",
    "    pred_original = predict_original(w, x)\n",
    "    pred_optimized = predict_optimized(s, W, x)\n",
    "    if pred_original != pred_optimized:\n",
    "        mismatches += 1\n",
    "\n",
    "print(f\"Prediction mismatches in {num_checks} samples: {mismatches}\")\n",
    "\n",
    "# Compare top features\n",
    "top_features_original = sorted(w.items(), key=lambda x: abs(x[1]), reverse=True)[:10]\n",
    "top_features_optimized = sorted(((k, v*s) for k, v in W.items()), key=lambda x: abs(x[1]), reverse=True)[:10]\n",
    "\n",
    "print(\"\\nTop 10 features (original):\")\n",
    "for feature, weight in top_features_original:\n",
    "    print(f\"{feature}: {weight:.6f}\")\n",
    "\n",
    "print(\"\\nTop 10 features (optimized):\")\n",
    "for feature, weight in top_features_optimized:\n",
    "    print(f\"{feature}: {weight:.6f}\")\n",
    "    \n",
    "error = classification_error(w, X_train, y_train)\n",
    "#print(f\"Classification error: {error:.4f}\")\n",
    "lambda_range = np.logspace(-6, 0, num=7)\n",
    "results = search_lambda(X_train, y_train, X_val, y_val, lambda_range)\n",
    "\n",
    "lambda_values, val_errors = zip(*results)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogx(lambda_values, val_errors, 'bo-')\n",
    "plt.xlabel('Regularization Parameter (Î»)')\n",
    "plt.ylabel('Validation Error')\n",
    "plt.title('Validation Error vs Regularization Parameter')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Find the best lambda from this search\n",
    "best_lambda, best_error = min(results, key=lambda x: x[1])\n",
    "print(f\"\\nBest Î» from initial search: {best_lambda:.6f}\")\n",
    "print(f\"Best validation error: {best_error:.4f}\")\n",
    "\n",
    "# Refined search around the best lambda\n",
    "refined_lambda_range = np.logspace(np.log10(best_lambda/10), np.log10(best_lambda*10), num=10)\n",
    "refined_results = search_lambda(X_train, y_train, X_val, y_val, refined_lambda_range)\n",
    "\n",
    "# Plot refined results\n",
    "refined_lambda_values, refined_val_errors = zip(*refined_results)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogx(refined_lambda_values, refined_val_errors, 'ro-')\n",
    "plt.xlabel('Regularization Parameter (Î»)')\n",
    "plt.ylabel('Validation Error')\n",
    "plt.title('Refined Search: Validation Error vs Regularization Parameter')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "best_lambda_refined, best_error_refined = min(refined_results, key=lambda x: x[1])\n",
    "print(f\"\\nBest Î» from refined search: {best_lambda_refined:.6f}\")\n",
    "print(f\"Best validation error: {best_error_refined:.4f}\")\n",
    "\n",
    "# Train the final model with the best lambda\n",
    "s_final, W_final = pegasos_sparse_optimized(X_train, y_train, best_lambda_refined)\n",
    "\n",
    "# Compute the test error\n",
    "X_test = [bow(review[:-1]) for review in val_data]  # Using validation data as test data\n",
    "y_test = [review[-1] for review in val_data]\n",
    "test_error = classification_error({k: v*s_final for k, v in W_final.items()}, X_test, y_test)\n",
    "print(f\"\\nFinal test error with best Î»: {test_error:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b48262",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import Counter\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "def read_data(file):\n",
    "    with open(file, 'r') as f:\n",
    "        lines = f.read().split(' ')\n",
    "    symbols = '${}()[].,:;+-*/&|<>=~\" '\n",
    "    words = [word.translate(str.maketrans(\"\", \"\", symbols)).strip() for word in lines]\n",
    "    return list(filter(None, words))\n",
    "\n",
    "def folder_list(path, label):\n",
    "    return [read_data(os.path.join(path, file)) + [label] for file in os.listdir(path)]\n",
    "\n",
    "def load_and_shuffle_data():\n",
    "    pos_path = \"/Users/tapankhaladkar/Machine Learning/hw2/data/pos\"\n",
    "    neg_path = \"/Users/tapankhaladkar/Machine Learning/hw2/data/neg\"\n",
    "    data = folder_list(pos_path, 1) + folder_list(neg_path, -1)\n",
    "    random.shuffle(data)\n",
    "    return data\n",
    "\n",
    "def bow(words):\n",
    "    return dict(Counter(words))\n",
    "\n",
    "# Helper functions for sparse vectors\n",
    "def dotProduct(d1, d2):\n",
    "    return sum(d1.get(f, 0) * v for f, v in d2.items())\n",
    "\n",
    "def increment(d1, scale, d2):\n",
    "    for f, v in d2.items():\n",
    "        d1[f] = d1.get(f, 0) + v * scale\n",
    "        \n",
    "def pegasos_sparse(X, y, lambda_param, max_epochs=1000, tolerance=0.001):\n",
    "    n = len(X)\n",
    "    w = {}  \n",
    "    t = 0  \n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        misclassified = 0\n",
    "        w_previous = copy.deepcopy(w) \n",
    "        \n",
    "        data = list(zip(X, y))\n",
    "        random.shuffle(data)\n",
    "        \n",
    "        for x_i, y_i in data:\n",
    "            t += 1\n",
    "            eta = 1 / (lambda_param * t)\n",
    "            \n",
    "            if dotProduct(w, x_i) * y_i < 1:\n",
    "                increment(w, -lambda_param * eta, w)  # w = (1 - eta * lambda) * w\n",
    "                increment(w, eta * y_i, x_i)  # w += eta * y_i * x_i\n",
    "                misclassified += 1\n",
    "            else:\n",
    "                increment(w, -lambda_param * eta, w)  # w = (1 - eta * lambda) * w\n",
    "        \n",
    "        error_rate = misclassified / n\n",
    "        if error_rate < tolerance:\n",
    "            print(f\"Converged after {epoch + 1} epochs (Error rate: {error_rate:.6f})\")\n",
    "            break\n",
    "        \n",
    "        weight_change = sum((w.get(k, 0) - w_previous.get(k, 0))**2 for k in set(w) | set(w_previous))\n",
    "        if weight_change < tolerance:\n",
    "            print(f\"Converged after {epoch + 1} epochs (Weight change: {weight_change:.6f})\")\n",
    "            break\n",
    "    \n",
    "    return w\n",
    "\n",
    "\n",
    "# Pegasos algorithm implementations\n",
    "def pegasos_sparse_optimized(X, y, lambda_param, max_epochs=1000, tolerance=0.001):\n",
    "    n = len(X)\n",
    "    s = 1  \n",
    "    W = {}  \n",
    "    t = 2  \n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        misclassified = 0\n",
    "        \n",
    "        data = list(zip(X, y))\n",
    "        random.shuffle(data)\n",
    "        \n",
    "        for x_i, y_i in data:\n",
    "            eta = 1 / (lambda_param * t)\n",
    "            \n",
    "            s = s * (1 - eta * lambda_param)\n",
    "            \n",
    "            if s == 0:\n",
    "                s = 1\n",
    "                W.clear()  \n",
    "            else:\n",
    "                if y_i * dotProduct(W, x_i) < 1/s:\n",
    "                    increment(W, (eta * y_i) / s, x_i)\n",
    "                    misclassified += 1\n",
    "            \n",
    "            t += 1\n",
    "        \n",
    "        error_rate = misclassified / n\n",
    "        if error_rate < tolerance:\n",
    "            print(f\"Converged after {epoch + 1} epochs (Error rate: {error_rate:.6f})\")\n",
    "            break\n",
    "    \n",
    "    return s, W\n",
    "\n",
    "def compute_norm(w):\n",
    "    return sum(v**2 for v in w.values())**0.5\n",
    "\n",
    "def predict_original(w, x):\n",
    "    return 1 if dotProduct(w, x) >= 0 else -1\n",
    "\n",
    "def predict_optimized(s, W, x):\n",
    "    return 1 if dotProduct(W, x) >= 0 else -1\n",
    "\n",
    "\n",
    "# Evaluation functions\n",
    "def classification_error(w, X, y):\n",
    "    return sum(1 for x_i, y_i in zip(X, y) if y_i * dotProduct(w, x_i) < 0) / len(y)\n",
    "\n",
    "\n",
    "def search_optimal_lambda(X_train, y_train, X_val, y_val, max_epochs=1000, tolerance=0.001):\n",
    "    lambdas = np.logspace(-5, 1, num=10)  # From 1e-5 to 10\n",
    "    errors = []\n",
    "\n",
    "    for lambda_param in lambdas:\n",
    "        print(f\"Testing lambda: {lambda_param:.5f}\")\n",
    "        s, W = pegasos_sparse_optimized(X_train, y_train, lambda_param, max_epochs, tolerance)\n",
    "        error = classification_error({k: v * s for k, v in W.items()}, X_val, y_val)\n",
    "        print(f\"Validation error: {error:.4f}\")\n",
    "        errors.append(error)\n",
    "\n",
    "    return lambdas, errors\n",
    "\n",
    "\n",
    "def plot_errors(lambdas, errors):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(lambdas, errors, marker='o', linestyle='-', color='b')\n",
    "    plt.xscale('log')  # Log scale for lambda\n",
    "    plt.xlabel('Regularization Parameter (Î»)')\n",
    "    plt.ylabel('Classification Error')\n",
    "    plt.title('Classification Error vs Regularization Parameter (Î»)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    all_data = load_and_shuffle_data()\n",
    "    train_data = all_data[:1500]\n",
    "    val_data = all_data[1500:]\n",
    "    \n",
    "    X_train = [bow(review[:-1]) for review in train_data]\n",
    "    y_train = [review[-1] for review in train_data]\n",
    "    X_val = [bow(review[:-1]) for review in val_data]\n",
    "    y_val = [review[-1] for review in val_data]\n",
    "\n",
    "    print(f\"Number of training examples: {len(X_train)}\")\n",
    "    print(f\"Number of validation examples: {len(X_val)}\")\n",
    "    print(f\"Sample training example:\\n{list(X_train[0].items())[:5]}...\")\n",
    "    print(f\"Corresponding label: {y_train[0]}\")\n",
    "    \n",
    "    max_epochs = 2\n",
    "    lambda_param = 0.1\n",
    "\n",
    "    start_time = time.time()\n",
    "    w = pegasos_sparse(X_train, y_train, lambda_param, max_epochs)\n",
    "    original_time = time.time() - start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    s, W = pegasos_sparse_optimized(X_train, y_train, lambda_param, max_epochs)\n",
    "    optimized_time = time.time() - start_time\n",
    "\n",
    "    norm_original = compute_norm(w)\n",
    "    norm_optimized = s * compute_norm(W)\n",
    "\n",
    "    print(f\"Original Pegasos runtime: {original_time:.4f} seconds\")\n",
    "    print(f\"Optimized Pegasos runtime: {optimized_time:.4f} seconds\")\n",
    "    print(f\"Norm of original weight vector: {norm_original:.6f}\")\n",
    "    print(f\"Norm of optimized weight vector: {norm_optimized:.6f}\")\n",
    "    print(f\"Relative difference in norms: {abs(norm_original - norm_optimized) / norm_original:.6f}\")\n",
    "    \n",
    "    error_original = classification_error(w, X_val, y_val)\n",
    "    error_optimized = classification_error({k: v*s for k, v in W.items()}, X_val, y_val)\n",
    "    print(f\"Original Pegasos classification error: {error_original:.4f}\")\n",
    "    print(f\"Optimized Pegasos classification error: {error_optimized:.4f}\")\n",
    "\n",
    "    \n",
    "    lambdas, errors = search_optimal_lambda(X_train, y_train, X_val, y_val)\n",
    "    plot_errors(lambdas, errors)\n",
    "\n",
    "    # Find the best lambda and its corresponding error\n",
    "    best_lambda = lambdas[np.argmin(errors)]\n",
    "    best_error = min(errors)\n",
    "\n",
    "    print(f\"Best regularization parameter (Î»): {best_lambda:.5f}\")\n",
    "    print(f\"Minimum classification error: {best_error:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90ba79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 1500\n",
      "Number of validation examples: 500\n",
      "Sample training example:\n",
      "[('walken', 3), ('stars', 1), ('as', 2), ('a', 13), ('mobster', 2)]...\n",
      "Corresponding label: -1\n",
      "Original Pegasos runtime: 5.7702 seconds\n",
      "Optimized Pegasos runtime: 0.1748 seconds\n",
      "Norm of original weight vector: 4.290707\n",
      "Norm of optimized weight vector: 4.379578\n",
      "Relative difference in norms: 0.020712\n",
      "Original Pegasos classification error: 0.4820\n",
      "Optimized Pegasos classification error: 0.4800\n",
      "Testing lambda: 0.00001\n",
      "Converged after 25 epochs (Error rate: 0.000000)\n",
      "Validation error: 0.1660\n",
      "Testing lambda: 0.00005\n",
      "Converged after 24 epochs (Error rate: 0.000000)\n",
      "Validation error: 0.1560\n",
      "Testing lambda: 0.00022\n",
      "Converged after 27 epochs (Error rate: 0.000000)\n",
      "Validation error: 0.1660\n",
      "Testing lambda: 0.00100\n",
      "Converged after 35 epochs (Error rate: 0.000667)\n",
      "Validation error: 0.1820\n",
      "Testing lambda: 0.00464\n",
      "Converged after 28 epochs (Error rate: 0.000000)\n",
      "Validation error: 0.1560\n",
      "Testing lambda: 0.02154\n",
      "Validation error: 0.1600\n",
      "Testing lambda: 0.10000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import Counter\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "def read_data(file):\n",
    "    with open(file, 'r') as f:\n",
    "        lines = f.read().split(' ')\n",
    "    symbols = '${}()[].,:;+-*/&|<>=~\" '\n",
    "    words = [word.translate(str.maketrans(\"\", \"\", symbols)).strip() for word in lines]\n",
    "    return list(filter(None, words))\n",
    "\n",
    "def folder_list(path, label):\n",
    "    return [read_data(os.path.join(path, file)) + [label] for file in os.listdir(path)]\n",
    "\n",
    "def load_and_shuffle_data():\n",
    "    pos_path = \"/Users/tapankhaladkar/Machine Learning/hw2/data/pos\"\n",
    "    neg_path = \"/Users/tapankhaladkar/Machine Learning/hw2/data/neg\"\n",
    "    data = folder_list(pos_path, 1) + folder_list(neg_path, -1)\n",
    "    random.shuffle(data)\n",
    "    return data\n",
    "\n",
    "def bow(words):\n",
    "    return dict(Counter(words))\n",
    "\n",
    "# Helper functions for sparse vectors\n",
    "def dotProduct(d1, d2):\n",
    "    return sum(d1.get(f, 0) * v for f, v in d2.items())\n",
    "\n",
    "def increment(d1, scale, d2):\n",
    "    for f, v in d2.items():\n",
    "        d1[f] = d1.get(f, 0) + v * scale\n",
    "        \n",
    "def pegasos_sparse(X, y, lambda_param, max_epochs=1000, tolerance=0.001):\n",
    "    n = len(X)\n",
    "    w = {}  \n",
    "    t = 0  \n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        misclassified = 0\n",
    "        w_previous = copy.deepcopy(w) \n",
    "        \n",
    "        data = list(zip(X, y))\n",
    "        random.shuffle(data)\n",
    "        \n",
    "        for x_i, y_i in data:\n",
    "            t += 1\n",
    "            eta = 1 / (lambda_param * t)\n",
    "            \n",
    "            if dotProduct(w, x_i) * y_i < 1:\n",
    "                increment(w, -lambda_param * eta, w)  # w = (1 - eta * lambda) * w\n",
    "                increment(w, eta * y_i, x_i)  # w += eta * y_i * x_i\n",
    "                misclassified += 1\n",
    "            else:\n",
    "                increment(w, -lambda_param * eta, w)  # w = (1 - eta * lambda) * w\n",
    "        \n",
    "        error_rate = misclassified / n\n",
    "        if error_rate < tolerance:\n",
    "            print(f\"Converged after {epoch + 1} epochs (Error rate: {error_rate:.6f})\")\n",
    "            break\n",
    "        \n",
    "        weight_change = sum((w.get(k, 0) - w_previous.get(k, 0))**2 for k in set(w) | set(w_previous))\n",
    "        if weight_change < tolerance:\n",
    "            print(f\"Converged after {epoch + 1} epochs (Weight change: {weight_change:.6f})\")\n",
    "            break\n",
    "    \n",
    "    return w\n",
    "\n",
    "\n",
    "# Pegasos algorithm implementations\n",
    "def pegasos_sparse_optimized(X, y, lambda_param, max_epochs=1000, tolerance=0.001):\n",
    "    n = len(X)\n",
    "    s = 1  \n",
    "    W = {}  \n",
    "    t = 2  \n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        misclassified = 0\n",
    "        \n",
    "        data = list(zip(X, y))\n",
    "        random.shuffle(data)\n",
    "        \n",
    "        for x_i, y_i in data:\n",
    "            eta = 1 / (lambda_param * t)\n",
    "            \n",
    "            s = s * (1 - eta * lambda_param)\n",
    "            \n",
    "            if s == 0:\n",
    "                s = 1\n",
    "                W.clear()  \n",
    "            else:\n",
    "                if y_i * dotProduct(W, x_i) < 1/s:\n",
    "                    increment(W, (eta * y_i) / s, x_i)\n",
    "                    misclassified += 1\n",
    "            \n",
    "            t += 1\n",
    "        \n",
    "        error_rate = misclassified / n\n",
    "        if error_rate < tolerance:\n",
    "            print(f\"Converged after {epoch + 1} epochs (Error rate: {error_rate:.6f})\")\n",
    "            break\n",
    "    \n",
    "    return s, W\n",
    "\n",
    "def compute_norm(w):\n",
    "    return sum(v**2 for v in w.values())**0.5\n",
    "\n",
    "def predict_original(w, x):\n",
    "    return 1 if dotProduct(w, x) >= 0 else -1\n",
    "\n",
    "def predict_optimized(s, W, x):\n",
    "    return 1 if dotProduct(W, x) >= 0 else -1\n",
    "\n",
    "\n",
    "# Evaluation functions\n",
    "def classification_error(w, X, y):\n",
    "    return sum(1 for x_i, y_i in zip(X, y) if y_i * dotProduct(w, x_i) < 0) / len(y)\n",
    "\n",
    "\n",
    "def search_optimal_lambda(X_train, y_train, X_val, y_val, max_epochs=1000, tolerance=0.001):\n",
    "    lambdas = np.logspace(-5, 1, num=10)  # From 1e-5 to 10\n",
    "    errors = []\n",
    "\n",
    "    for lambda_param in lambdas:\n",
    "        print(f\"Testing lambda: {lambda_param:.5f}\")\n",
    "        s, W = pegasos_sparse_optimized(X_train, y_train, lambda_param, max_epochs, tolerance)\n",
    "        error = classification_error({k: v * s for k, v in W.items()}, X_val, y_val)\n",
    "        print(f\"Validation error: {error:.4f}\")\n",
    "        errors.append(error)\n",
    "\n",
    "    return lambdas, errors\n",
    "\n",
    "\n",
    "def plot_errors(lambdas, errors):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(lambdas, errors, marker='o', linestyle='-', color='b')\n",
    "    plt.xscale('log')  # Log scale for lambda\n",
    "    plt.xlabel('Regularization Parameter (Î»)')\n",
    "    plt.ylabel('Classification Error')\n",
    "    plt.title('Classification Error vs Regularization Parameter (Î»)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    all_data = load_and_shuffle_data()\n",
    "    train_data = all_data[:1500]\n",
    "    val_data = all_data[1500:]\n",
    "    \n",
    "    X_train = [bow(review[:-1]) for review in train_data]\n",
    "    y_train = [review[-1] for review in train_data]\n",
    "    X_val = [bow(review[:-1]) for review in val_data]\n",
    "    y_val = [review[-1] for review in val_data]\n",
    "\n",
    "    print(f\"Number of training examples: {len(X_train)}\")\n",
    "    print(f\"Number of validation examples: {len(X_val)}\")\n",
    "    print(f\"Sample training example:\\n{list(X_train[0].items())[:5]}...\")\n",
    "    print(f\"Corresponding label: {y_train[0]}\")\n",
    "    \n",
    "    max_epochs = 2\n",
    "    lambda_param = 0.1\n",
    "\n",
    "    start_time = time.time()\n",
    "    w = pegasos_sparse(X_train, y_train, lambda_param, max_epochs)\n",
    "    original_time = time.time() - start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    s, W = pegasos_sparse_optimized(X_train, y_train, lambda_param, max_epochs)\n",
    "    optimized_time = time.time() - start_time\n",
    "\n",
    "    norm_original = compute_norm(w)\n",
    "    norm_optimized = s * compute_norm(W)\n",
    "\n",
    "    print(f\"Original Pegasos runtime: {original_time:.4f} seconds\")\n",
    "    print(f\"Optimized Pegasos runtime: {optimized_time:.4f} seconds\")\n",
    "    print(f\"Norm of original weight vector: {norm_original:.6f}\")\n",
    "    print(f\"Norm of optimized weight vector: {norm_optimized:.6f}\")\n",
    "    print(f\"Relative difference in norms: {abs(norm_original - norm_optimized) / norm_original:.6f}\")\n",
    "    \n",
    "    error_original = classification_error(w, X_val, y_val)\n",
    "    error_optimized = classification_error({k: v*s for k, v in W.items()}, X_val, y_val)\n",
    "    print(f\"Original Pegasos classification error: {error_original:.4f}\")\n",
    "    print(f\"Optimized Pegasos classification error: {error_optimized:.4f}\")\n",
    "\n",
    "    \n",
    "    lambdas, errors = search_optimal_lambda(X_train, y_train, X_val, y_val)\n",
    "    plot_errors(lambdas, errors)\n",
    "\n",
    "    best_lambda = lambdas[np.argmin(errors)]\n",
    "    best_error = min(errors)\n",
    "\n",
    "    print(f\"Best regularization parameter (Î»): {best_lambda:.5f}\")\n",
    "    print(f\"Minimum classification error: {best_error:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7beb94e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
