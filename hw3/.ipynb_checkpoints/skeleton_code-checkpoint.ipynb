{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "772132a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, clone\n",
    "\n",
    "class OneVsAllClassifier(BaseEstimator, ClassifierMixin):  \n",
    "    \"\"\"\n",
    "    One-vs-all classifier\n",
    "    We assume that the classes will be the integers 0,..,(n_classes-1).\n",
    "    We assume that the estimator provided to the class, after fitting, has a \"decision_function\" that \n",
    "    returns the score for the positive class.\n",
    "    \"\"\"\n",
    "    def __init__(self, estimator, n_classes):      \n",
    "        \"\"\"\n",
    "        Constructed with the number of classes and an estimator (e.g. an\n",
    "        SVM estimator from sklearn)\n",
    "        @param estimator : binary base classifier used\n",
    "        @param n_classes : number of classes\n",
    "        \"\"\"\n",
    "        self.n_classes = n_classes \n",
    "        self.estimators = [clone(estimator) for _ in range(n_classes)]\n",
    "        self.fitted = False\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        This should fit one classifier for each class.\n",
    "        self.estimators[i] should be fit on class i vs rest\n",
    "        @param X: array-like, shape = [n_samples,n_features], input data\n",
    "        @param y: array-like, shape = [n_samples,] class labels\n",
    "        @return returns self\n",
    "        \"\"\"\n",
    "        if y is None:\n",
    "            raise ValueError(\"y cannot be None\")\n",
    "    \n",
    "        for class_idx in range(self.n_classes):\n",
    "            binary_labels = np.where(y == class_idx, 1, -1)\n",
    "            self.estimators[class_idx].fit(X, binary_labels)\n",
    "        \n",
    "        self.fitted = True  \n",
    "        return self   \n",
    "\n",
    "    def decision_function(self, X):\n",
    "        \"\"\"\n",
    "        Returns the score of each input for each class. Assumes\n",
    "        that the given estimator also implements the decision_function method (which sklearn SVMs do), \n",
    "        and that fit has been called.\n",
    "        @param X : array-like, shape = [n_samples, n_features] input data\n",
    "        @return array-like, shape = [n_samples, n_classes]\n",
    "        \"\"\"\n",
    "        if not self.fitted:\n",
    "            raise RuntimeError(\"You must train classifer before predicting data.\")\n",
    "\n",
    "        if not hasattr(self.estimators[0], \"decision_function\"):\n",
    "            raise AttributeError(\n",
    "                \"Base estimator doesn't have a decision_function attribute.\")\n",
    "        \n",
    "        scores = np.zeros((X.shape[0], self.n_classes))\n",
    "        for class_idx in range(self.n_classes):\n",
    "            scores[:, class_idx] = self.estimators[class_idx].decision_function(X)\n",
    "            \n",
    "        return scores\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the class with the highest score.\n",
    "        @param X: array-like, shape = [n_samples,n_features] input data\n",
    "        @returns array-like, shape = [n_samples,] the predicted classes for each input\n",
    "        \"\"\"\n",
    "        scores = self.decision_function(X)\n",
    "        \n",
    "        return np.argmax(scores, axis=1)\n",
    "\n",
    "\n",
    "def zeroOne(y,a) :\n",
    "    '''\n",
    "    Computes the zero-one loss.\n",
    "    @param y: output class\n",
    "    @param a: predicted class\n",
    "    @return 1 if different, 0 if same\n",
    "    '''\n",
    "    return int(y != a)\n",
    "\n",
    "def featureMap(X,y,num_classes) :\n",
    "    '''\n",
    "    Computes the class-sensitive features.\n",
    "    @param X: array-like, shape = [n_samples,n_inFeatures] or [n_inFeatures,], input features for input data\n",
    "    @param y: a target class (in range 0,..,num_classes-1)\n",
    "    @return array-like, shape = [n_samples,n_outFeatures], the class sensitive features for class y\n",
    "    '''\n",
    "    #The following line handles X being a 1d-array or a 2d-array\n",
    "    num_samples, num_inFeatures = (1,X.shape[0]) if len(X.shape) == 1 else (X.shape[0],X.shape[1])\n",
    "    if len(X.shape) == 1:\n",
    "        X = X.reshape(1, -1)\n",
    "    \n",
    "    # Create output array with zeros\n",
    "    # Total features = num_classes * num_inFeatures (one block per class)\n",
    "    out_features = np.zeros((num_samples, num_classes * num_inFeatures))\n",
    "    \n",
    "    # For each sample, place the input features in the block corresponding to class y\n",
    "    for i in range(num_samples):\n",
    "        # Calculate starting and ending indices for the block\n",
    "        \n",
    "        # Place the features in the appropriate block\n",
    "        num_outFeatures[i, start_idx:end_idx] = X[i]\n",
    "    \n",
    "    return num_outFeatures\n",
    "\n",
    "def sgd(X, y, num_outFeatures, subgd, eta = 0.1, T = 10000):\n",
    "    '''\n",
    "    Runs subgradient descent, and outputs resulting parameter vector.\n",
    "    @param X: array-like, shape = [n_samples,n_features], input training data \n",
    "    @param y: array-like, shape = [n_samples,], class labels\n",
    "    @param num_outFeatures: number of class-sensitive features\n",
    "    @param subgd: function taking x,y,w and giving subgradient of objective\n",
    "    @param eta: learning rate for SGD\n",
    "    @param T: maximum number of iterations\n",
    "    @return: vector of weights\n",
    "    '''\n",
    "    num_samples = X.shape[0]\n",
    "    w = np.zeros(num_outFeatures)\n",
    "    \n",
    "   \n",
    "    for t in range(T):\n",
    "        # Select a random training example\n",
    "        i = np.random.randint(num_samples)\n",
    "        x_i = X[i]\n",
    "        y_i = y[i]\n",
    "\n",
    "        # Compute the learning rate for this iteration\n",
    "        # Using a decreasing learning rate schedule: eta / sqrt(t+1)\n",
    "        eta_t = eta / np.sqrt(t + 1)\n",
    "\n",
    "        # Compute subgradient using the provided subgradient function\n",
    "        subgradient = subgd(x_i, y_i, w)\n",
    "\n",
    "        # Update weights using the subgradient\n",
    "        w = w - eta_t * subgradient\n",
    "\n",
    "    return w\n",
    "\n",
    "class MulticlassSVM(BaseEstimator, ClassifierMixin):\n",
    "    '''\n",
    "    Implements a Multiclass SVM estimator.\n",
    "    '''\n",
    "    def __init__(self, num_outFeatures, lam=1.0, num_classes=3, Delta=zeroOne, Psi=featureMap):       \n",
    "        '''\n",
    "        Creates a MulticlassSVM estimator.\n",
    "        @param num_outFeatures: number of class-sensitive features produced by Psi\n",
    "        @param lam: l2 regularization parameter\n",
    "        @param num_classes: number of classes (assumed numbered 0,..,num_classes-1)\n",
    "        @param Delta: class-sensitive loss function taking two arguments (i.e., target margin)\n",
    "        @param Psi: class-sensitive feature map taking two arguments\n",
    "        '''\n",
    "        self.num_outFeatures = num_outFeatures\n",
    "        self.lam = lam\n",
    "        self.num_classes = num_classes\n",
    "        self.Delta = Delta\n",
    "        self.Psi = lambda X,y : Psi(X,y,num_classes)\n",
    "        self.fitted = False\n",
    "    \n",
    "    def subgradient(self,x,y,w):\n",
    "        '''\n",
    "        Computes the subgradient at a given data point x,y\n",
    "        @param x: sample input\n",
    "        @param y: sample class\n",
    "        @param w: parameter vector\n",
    "        @return returns subgradient vector at given x,y,w\n",
    "        '''\n",
    "        psi_y = self.Psi(x, y)\n",
    "    \n",
    "        # Initialize subgradient with regularization term (2Î»w)\n",
    "        subgrad = 2 * self.lam * w\n",
    "\n",
    "        # Find the class that gives maximum loss\n",
    "        max_loss = float('-inf')\n",
    "        max_class = None\n",
    "\n",
    "        # Check all possible classes\n",
    "        for y_prime in range(self.num_classes):\n",
    "            # Skip if it's the true class\n",
    "            if y_prime == y:\n",
    "                continue\n",
    "\n",
    "            # Compute feature map for current class\n",
    "            psi_y_prime = self.Psi(x, y_prime)\n",
    "\n",
    "            # Compute the loss for this class\n",
    "            loss = self.Delta(y, y_prime) + np.dot(w, psi_y_prime) - np.dot(w, psi_y)\n",
    "\n",
    "            if loss > max_loss:\n",
    "                max_loss = loss\n",
    "                max_class = y_prime\n",
    "\n",
    "        # If there is a class that gives positive loss\n",
    "        if max_loss > 0:\n",
    "            # Add the feature difference to the subgradient\n",
    "            subgrad += self.Psi(x, max_class) - psi_y\n",
    "\n",
    "        return subgrad\n",
    "\n",
    "        \n",
    "    def fit(self,X,y,eta=0.1,T=10000):\n",
    "        '''\n",
    "        Fits multiclass SVM\n",
    "        @param X: array-like, shape = [num_samples,num_inFeatures], input data\n",
    "        @param y: array-like, shape = [num_samples,], input classes\n",
    "        @param eta: learning rate for SGD\n",
    "        @param T: maximum number of iterations\n",
    "        @return returns self\n",
    "        '''\n",
    "        self.coef_ = sgd(X,y,self.num_outFeatures,self.subgradient,eta,T)\n",
    "        self.fitted = True\n",
    "        return self\n",
    "    \n",
    "    def decision_function(self, X):\n",
    "        '''\n",
    "        Returns the score on each input for each class. Assumes\n",
    "        that fit has been called.\n",
    "        @param X : array-like, shape = [n_samples, n_inFeatures]\n",
    "        @return array-like, shape = [n_samples, n_classes] giving scores for each sample,class pairing\n",
    "        '''\n",
    "        if not self.fitted:\n",
    "            raise RuntimeError(\"You must train classifier before predicting data.\")\n",
    "            \n",
    "        n_samples = X.shape[0]\n",
    "        scores = np.zeros((n_samples, self.num_classes))\n",
    "    \n",
    "    # For each sample and class\n",
    "        for i in range(n_samples):\n",
    "            for y in range(self.num_classes):\n",
    "                # Compute feature map for this class\n",
    "                psi = self.Psi(X[i], y)\n",
    "                # Compute score as dot product with weights\n",
    "                scores[i, y] = np.dot(self.coef_, psi)\n",
    "\n",
    "        return scores\n",
    "        \n",
    "            \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Predict the class with the highest score.\n",
    "        @param X: array-like, shape = [n_samples, n_inFeatures], input data to predict\n",
    "        @return array-like, shape = [n_samples,], class labels predicted for each data point\n",
    "        '''\n",
    "        scores = self.decision_function(X)\n",
    "        # Return class with highest score for each sample\n",
    "        return np.argmax(scores, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c900fb34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b52d40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d1f7d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
