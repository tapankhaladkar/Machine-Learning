\documentclass{article}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage[dvipsnames]{xcolor}
\usepackage{enumitem}
\usepackage{minted}
\input{math_commands}

\newcommand{\wipcom}[1]{\textcolor{red}{WIP: #1}}
\newcommand{\sol}[1]{\textcolor{gray}{sol: #1}}
% \newcommand{\sol}[1]{}
\newcommand{\nyuparagraph}[1]{\vspace{0.3cm}\textcolor{nyupurple}{\bf \large #1}\\}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\nll}{\rm NLL}
\newcommand{\redcolor}[1]{{\color{red} #1}}
% \newcommand{\redcolor}[1]{}



\pagestyle{empty} \addtolength{\textwidth}{1.0in}
\addtolength{\textheight}{0.5in} \addtolength{\oddsidemargin}{-0.5in}
\addtolength{\evensidemargin}{0.5in}
\newcommand{\ruleskip}{\bigskip\hrule\bigskip}
\newcommand{\nodify}[1]{{\sc #1}} \newcommand{\points}[1]{{\textbf{[#1
points]}}}

\newcommand{\bitem}{\begin{list}{$\bullet$}%
{\setlength{\itemsep}{0pt}\setlength{\topsep}{0pt}%
\setlength{\rightmargin}{0pt}}} \newcommand{\eitem}{\end{list}}

\definecolor{nyupurple}{RGB}{134, 0, 179}
\setlength{\parindent}{0pt} \setlength{\parskip}{0.5ex}

\DeclareUnicodeCharacter{2212}{-}

\theoremstyle{plain}
\newtheorem*{thm*}{\protect\theoremname}
\theoremstyle{definition}
\newtheorem*{defn*}{\protect\definitionname}

\begin{document}
\newcounter{saveenum}

\pagestyle{myheadings} \markboth{}{\color{nyupurple} CSCI-GA 2565 - Fall 2023}

\begin{center}
{\Large
Homework 4: Decision Trees, Boosting, and Neural Networks
} 
\end{center}

{
{ \color{nyupurple} \textbf{Due:} Tuesday, Dec 3rd, 2024 at 11:59AM EST} 
} 

\textbf{Instructions: }Your answers to the questions below, including plots and mathematical work, should be submitted as a single PDF file.  It's preferred that you write your answers using software that typesets mathematics (e.g.LaTeX, LyX, or MathJax via iPython), though if you need to you may scan handwritten work.  You may find the \href{https://github.com/gpoore/minted}{minted} package convenient for including source code in your LaTeX document.  If you are using LyX, then the \href{https://en.wikibooks.org/wiki/LaTeX/Source_Code_Listings}{listings} package tends to work better. {\bf The optional problems should not take you too much time and help you navigate the material, consider taking a shot at them.}

\ruleskip


% \pagestyle{fancy} \lhead{\includegraphics[width=4cm]{../figures/logo.PNG}} \rhead{}


\section{Decision Tree Implementation}

In this problem we'll implement decision trees for both classification
and regression. The strategy will be to implement a generic class,
called \texttt{Decision\_Tree}, which we'll supply with the loss function
we want to use to make node splitting decisions, as well as the estimator
we'll use to come up with the prediction associated with each leaf
node. For classification, this prediction could be a vector of probabilities,
but for simplicity we'll just consider hard classifications here.
We'll work with the classification and regression data sets from previous
assignments.
\begin{enumerate}
\setcounter{enumi}{\value{saveenum}}
\item Complete either the \texttt{compute\_entropy} or \texttt{compute\_gini}
functions.

\item Complete the class \texttt{Decision\_Tree}, given in
the skeleton code. The intended implementation is as follows: Each
object of type \texttt{Decision\_Tree} represents a single node of
the tree. The depth of that node is represented by the variable self.depth,
with the root node having depth 0. The main job of the fit function
is to decide, given the data provided, how to split the node or whether
it should remain a leaf node. If the node will split, then the splitting
feature and splitting value are recorded, and the left and right subtrees
are fit on the relevant portions of the data. Thus tree-building is
a recursive procedure. We should have as many \texttt{Decision\_Tree}
objects as there are nodes in the tree. We will not implement pruning\textbf{
}here. Some additional details are given in the skeleton code.

\item Run the code provided that builds trees for the two-dimensional
classification data. Include the results. For debugging, you may want
to compare results with sklearn's decision tree (code provided in the skeleton code). For visualization,
you'll need to install \texttt{graphviz}.

\item  Complete the function \texttt{mean\_absolute\_deviation\_around\_median}
(MAE). Use the code provided to fit the \texttt{Regression\_Tree} to
the krr dataset using both the MAE loss and median predictions. Include the plots for the 6 fits with max depth varying from 1 to 6.

\setcounter{saveenum}{\value{enumi}}
\end{enumerate}

\section{Ensembling}

Recall the general gradient boosting algorithm
% \footnote{Besides the lecture slides, you can find an accessible discussion
% of this approach in \url{http://www.saedsayad.com/docs/gbm2.pdf},
% in one of the original references \url{http://statweb.stanford.edu/~jhf/ftp/trebst.pdf},
% and in this review paper \url{http://web.stanford.edu/~hastie/Papers/buehlmann.pdf}. }
, for a given loss function $\ell$ and a hypothesis space $\cf$
of regression functions (i.e. functions mapping from the input space
to $\reals$): 
\begin{enumerate}
\setcounter{enumi}{\value{saveenum}}
\item[0:] Initialize $f_{0}(x)=0$. 
\item[1:] For $m=1$ to $M$:

\begin{enumerate}
\item Compute: 
\[
{\bf g}_{m}=\left( \frac{\partial}{\partial f_{m-1}(x_{j})}\sum_{i=1}^{n}\ell\left(y_{i},f_{m-1}(x_{i})\right)\right)_{j=1}^{n}
\]
\item Fit regression model to $-{\bf g}_{m}$: 
\[
h_{m}=\argmin_{h\in\cf}\sum_{i=1}^{n}\left(\left(-{\bf g}_{m}\right)_{i}-h(x_{i})\right)^{2}.
\]
\item Choose fixed step size $\nu_{m}=\nu\in(0,1]$, or take 
\[
\nu_{m}=\argmin_{\nu>0}\sum_{i=1}^{n}\ell\left(y_{i},f_{m-1}(x_{i})+\nu h_{m}(x_{i})\right).
\]
\item Take the step: 
\[
f_{m}(x)=f_{m-1}(x)+\nu_{m}h_{m}(x)
\]
\end{enumerate}
\item[3:] Return $f_{M}$. 
\end{enumerate}


This method goes by many names, including gradient boosting machines
(GBM), generalized boosting models (GBM), AnyBoost, and gradient boosted
regression trees (GBRT), among others. One of the nice aspects
of gradient boosting is that it can be applied to any problem with
a subdifferentiable loss function.

\nyuparagraph{Gradient Boosting Regression Implementation}
First we'll keep things simple and consider the standard regression setting with square loss. In this case, we have $\cy=\reals$, our
loss function is given by $
\ell(\hat{y},y)=1/2\left(\hat{y}-y\right)^{2}$,
and at the $m$'th round of gradient boosting, we
have
\[
h_{m}=\argmin_{h\in\cf}\sum_{i=1}^{n}\left[\left(y_{i}-f_{m-1}(x_{i})\right)-h(x_{i})\right]^{2}.
\]
You can derive the above equation using (a) and (b). 

\begin{enumerate}
\setcounter{enumi}{\value{saveenum}}
  
\item Complete the \texttt{gradient\_boosting} class. As the base regression
algorithm to compute the argmin, you should use sklearn's regression tree. You should use
the square loss for the tree splitting rule (\texttt{criterion} keyword argument) and use the default sklearn leaf prediction rule from the \texttt{predict} method \footnote{Examples of usage are given in the skeleton code to debug previous problems, and you can check the docs \url{https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html}}. We will also use a constant step size $\nu$.

\item Use the \texttt{Decision\_Tree} code provided to build gradient
boosting models on the regression data sets \texttt{krr-train.txt}, and include the plots generated. For debugging you can use the sklearn implementation of \texttt{GradientBoostingRegressor}\footnote{\url{https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html}}.
\setcounter{saveenum}{\value{enumi}}

\end{enumerate}

% \nyuparagraph{Classification of images with Gradient Boosting}
% In this problem we will consider the classification of MNIST, the dataset of handwritten digits images, with ensembles of trees. For simplicity, we only retain the `0' and '1' examples and perform binary classification.

% First we'll derive a special case of the general gradient
% boosting framework: BinomialBoost. 
% Let's consider the classification framework, where $\cy=\left\{ -1,1\right\} $.
% In lecture, we noted that AdaBoost corresponds to forward stagewise
% additive modeling with the exponential loss, and that the exponential
% loss is not very robust to outliers (i.e. outliers can have a large
% effect on the final prediction function). Instead, let's consider
% the logistic loss 
% \[
% \ell(m)=\ln\left(1+e^{-m}\right),
% \]
% where $m=yf(x)$ is the margin.

% \begin{enumerate}
%   \setcounter{enumi}{\value{saveenum}}
  
% \item Give the expression of the negative gradient step direction, or pseudo residual, $-{\bf g}_{m}$ for the logistic loss as a function of the prediction function $f_{m-1}$ at the previous iteration and the dataset points $\{(x_i, y_i)\}_{i=1}^n$. What is the dimension of $g_{m}$?\\
% \sol{
% $g_{m}$
% \[
% \textbf{g}_{m}=\left(\frac{-y_{i}}{1+e^{y_{i}f_{m-1}(x_{i})}}\right)_{i=1}^{n}
% \]\\
% Grading: 1pt
% }
% \item Write an expression for $h_{m}$ as an argmin over functions $h$ in $\cf$.\\
% \sol{
% \[
% h_{m}=\argmin_{h\in\cf}\sum_{i=1}^{n}\left(\left(-{\bf g}_{m}\right)_{i}-h(x_{i})\right)^{2}=\argmin_{h\in\cf}\sum_{i=1}^{n}\left[\left(\frac{y_{i}}{1+e^{y_{i}f_{m-1}(x_{i})}}\right)-h(x_{i})\right]^{2}.
% \]\\
% Grading: 1pt
% }
  
% \item Load the MNIST dataset using the helper preprocessing function in the skeleton code.Using the scikit learn implementation of \texttt{GradientBoostingClassifier}, with the logistic loss (\texttt{loss=`deviance'}) and trees of maximum depth 3, fit the data with 2, 5, 10, 100 and 200 iterations (estimators). Plot the train and test accurary as a function of the number of estimators.\\
% \sol{ipynb cell 22.\\
% Grading: 1pt for the plot.}

% \setcounter{saveenum}{\value{enumi}}
% \end{enumerate}

% \nyuparagraph{Classification of images with Random Forests (Optional)}
% \begin{enumerate}
%   \setcounter{enumi}{\value{saveenum}}
% \item Another type of ensembling method we discussed in class are random forests. Explain in your own words the construction principle of random forests.\\
% \sol{
%   Ensembling with trees trained in parallel on bootstrapped samples and only consider a subset of features for splitting at each tree node construction.\\
% Grading: 1pt
% }
% \item Using the scikit learn implementation of \texttt{RandomForestClassifier}\footnote{\url{https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\#sklearn.ensemble.RandomForestClassifier}},
% with the entropy loss (\texttt{criterion=`entropy'}) and trees of maximum depth 3, fit the preprocessed binary MNIST dataset with 2, 5, 10, 50, 100 and 200 estimators.\\
% \sol{ipynb cell 23.
% \\
% Grading: 1pt for the plot.}

% \item What general remark can you make on overfitting for Random Forests and Gradient Boosted Trees? Which method achieves the best train accuracy overall? Is this result expected? 
% Can you think of a practical disadvantage of the best performing method? How do the algorithms compare in term of test accuracy? \\
% \sol{
% %   Robust to overfitting. 
% ipynb cell 24.\\
%   Gradient boosting achieves better training accuracy: ensembling done sequentially to reduce the error, so result is expected. Disadvantage is that we cannot perform training in parallel, so method will be typically slower. RF achieves higher test accuracy when number of estimators is small but it is comparable at large number of estimators.\\
% Grading: 2 pts: 1 pt for discussion on train/test accuracy, and 1 pt for discussion/comparison on overfitting and practical advantages.
% }
% \setcounter{saveenum}{\value{enumi}}
% \end{enumerate}

\section{Neural Network Introduction}

There is no doubt that neural networks are a very important class
of machine learning models. Given the sheer number of people who are
achieving impressive results with neural networks, one might think
that it's relatively easy to get them working. This is a partly an
illusion. One reason so many people have success is that, thanks to
GitHub, they can copy the exact settings that others have used to
achieve success. In fact, in most cases they can start with ``pre-trained''
models that already work for a similar problem, and ``fine-tune''
them for their own purposes. It's far easier to tweak and improve
a working system than to get one working from scratch. If you create
a new model, you're kind of on your own to figure out how to get it
working: there's not much theory to guide you and the rules of thumb
do not always work. Understanding even the most basic questions, such
as the preferred variant of SGD to use for optimization, is still
a very active area of research.

One thing is clear, however: If you do need to start from scratch,
or debug a neural network model that doesn't seem to be learning,
it can be immensely helpful to understand the low-level details of
how your neural network works -- specifically, back-propagation.
With this assignment, you'll have the opportunity to linger on these
low-level implementation details. Every major neural network type
(RNNs, CNNs, Resnets, etc.) can be implemented using the basic framework
we'll develop in this assignment.

To help things along, Philipp Meerkamp, Pierre Garapon, and David Rosenberg
have designed a minimalist framework for computation graphs and put
together some support code. The intent is for you to read, or at least
skim, every line of code provided, so that you'll know you understand
all the crucial components and could, in theory, create your own from
scratch. In fact, creating your own computation graph framework from
scratch is highly encouraged -- you'll learn a lot. 

\section{Computation Graph Framework }

To get started, please read the \href{https://github.com/davidrosenberg/mlcourse/blob/gh-pages/Notebooks/computation-graph/computation-graph-framework.ipynb}{tutorial}
on the computation graph framework we'll be working with. (Note that
it renders better if you view it locally.) The use of computation
graphs is not specific to machine learning or neural networks. Computation
graphs are just a way to represent a function that facilitates efficient
computation of the function's values and its gradients with respect
to inputs. The tutorial takes this perspective, and there is very
little in it about machine learning, per se. 

To see how the framework can be used for machine learning tasks, we've
provided a full implementation of linear regression. You should start
by working your way through the \texttt{\_\_init\_\_} of the \texttt{LinearRegression}
class in \texttt{linear\_regression.py}. From there, you'll want to
review the node class definitions in \texttt{nodes.py}, and finally
the class \texttt{ComputationGraphFunction} in \texttt{graph.py}. \texttt{ComputationGraphFunction}
is where we repackage a raw computation graph into something that's
more friendly to work with for machine learning. The rest of \texttt{linear\_regression.py}
is fairly routine, but it illustrates how to interact with the \texttt{ComputationGraphFunction}.

As we've noted earlier in the course, getting gradient calculations
correct can be difficult. To help things along, we've provided two
functions that can be used to test the backward method of a node and
the overall gradient calculation of a \texttt{ComputationGraphFunction}.
The functions are in \texttt{test\_utils.py}, and it's recommended
that you review the tests provided for the linear regression implementation
in \texttt{linear\_regression.t.py}. (You can run these tests from
the command line with \texttt{python3 linear\_regression.t.py.}) The
functions actually doing the testing, \texttt{test\_node\_backward}
and \texttt{test\_ComputationGraphFunction}, may seem a bit intricate,
but they're implementing the exact same \texttt{gradient\_checker}
logic we saw in the second homework assignment.

Once you've understood how linear regression works in our framework,
you're ready to start implementing your own algorithms. To help you get started, please make sure you are able to execute the following commands:
\begin{itemize}
    \item cd /path/to/hw4
    \item python3 linear\_regression.py
    \item python3 linear\_regression.t.py
\end{itemize}

\section{Ridge Regression}

When moving to a new system, it's always good to start with something
familiar. But that's not the only reason we're doing ridge regression
in this homework. In ridge regression the
parameter vector is ``shared'', in the sense that it's used twice
in the objective function. In the computation graph, this can be seen
in the fact that the node for the parameter vector has two outgoing
edges. 
This sharing is common many popular neural networks (RNNs and CNNs), where it is often referred to as \emph{parameter tying}.

\texttt{ridge\_regression.py} provides a skeleton code
and \texttt{ridge\_regression.t.py} is a test code, which you should
eventually be able to pass.
\begin{enumerate}
\setcounter{enumi}{\value{saveenum}}
\item Complete the class \texttt{L2NormPenaltyNode} in \texttt{nodes.py}. If your code is correct, you should be able to pass test\_L2NormPenaltyNode in \texttt{ridge\_regression.t.py}. Please attach a screenshot that shows the test results for this question. 

\item Complete the class \texttt{SumNode} in \texttt{nodes.py}. If your code is correct, you should be able to pass test\_SumNode in \texttt{ridge\_regression.t.py}. Please attach a screenshot that shows the test results for this question.
\item Implement ridge regression with $w$ regularized and $b$ unregularized.
Do this by completing the \texttt{\_\_init\_\_} method in \texttt{ridge\_regression.py},
using the classes created above. When complete, you should be able
to pass the tests in \texttt{ridge\_regression.t.py}. Report the average
square error on the \textbf{training} set for the parameter settings
given in the \texttt{main()} function.
\setcounter{saveenum}{\value{enumi}}
\end{enumerate}

\section{Multilayer Perceptron}
Let's now turn to a multilayer perceptron (MLP)
with a single hidden layer and a square loss. We'll implement the
computation graph illustrated below:
\begin{center}
\includegraphics{./MLP-computation-graph}
\par\end{center}

The crucial new piece here is the nonlinear \textbf{hidden layer},
which is what makes the multilayer perceptron a significantly larger
hypothesis space than linear prediction functions.

\subsection{The standard non-linear layer}

The multilayer perceptron consists of a sequence of ``layers'' implementing
the following non-linear function
\[
h(x)=\sigma\left(Wx+b\right),
\]
where $x\in\reals^{d}$, $W\in\reals^{m\times d},$ and $b\in\reals^{m}$,
and where $m$ is often referred to as the number of\textbf{ hidden
units }or\textbf{ hidden nodes}. $\sigma$ is some non-linear function,
typically $\tanh$ or ReLU, applied element-wise to the argument of
$\sigma$. Referring to the computation graph illustration above,
we will implement this nonlinear layer with two nodes, one implementing
the affine transform $L=W_{1}x+b_{1}$, and the other implementing
the nonlinear function $h=\tanh(L)$. In this problem, we'll work
out how to implement the backward method for each of these nodes.

\nyuparagraph{The Affine Transformation}

In a general neural network, there may be quite a lot of computation
between any given affine transformation $Wx+b$ and the final objective
function value $J$. We will capture all of that in a function $f:\reals^{m}\to\reals$,
for which $J=f(Wx+b)$. Our goal is to find the partial derivative
of $J$ with respect to each element of $W$, namely $\partial J/\partial W_{ij}$,
as well as the partials $\partial J/\partial b_{i}$, for each element
of $b$. For convenience, let $y=Wx+b$, so we can write $J=f(y)$.
Suppose we have already computed the partial derivatives of $J$ with
respect to the entries of $y=\left(y_{1},\ldots,y_{m}\right)^{T}$,
namely $\frac{\partial J}{\partial y_{i}}$ for $i=1,\ldots,m$. Then
by the chain rule, we have
\[
\frac{\partial J}{\partial W_{ij}}=\sum_{r=1}^{m}\frac{\partial J}{\partial y_{r}}\frac{\partial y_{r}}{\partial W_{ij}}.
\]

\begin{enumerate}
\setcounter{enumi}{\value{saveenum}}
\item Show that $\frac{\partial J}{\partial W_{ij}}=\frac{\partial J}{\partial y_{i}}x_{j}$,
where $x=\left(x_{1},\ldots,x_{d}\right)^{T}$. {[}Hint: Although
not necessary, you might find it helpful to use the notation $\delta_{ij}=\begin{cases}
1 & i=j\\
0 & \text{else}
\end{cases}$. So, for examples, $\partial_{x_{j}}\left(\sum_{i=1}^{n}x_{i}^{2}\right)=2x_{i}\delta_{ij}=2x_{j}$.{]}

\item Now let's vectorize this. Let's write $\frac{\partial J}{\partial y}\in\reals^{m\times1}$
for the column vector whose $i$th entry is $\frac{\partial J}{\partial y_{i}}$.
Let's also define the matrix $\frac{\partial J}{\partial W}\in\reals^{m\times d}$,
whose $ij$'th entry is $\frac{\partial J}{\partial W_{ij}}$. Generally
speaking, we'll always take $\frac{\partial J}{\partial A}$ to be
an array of the same size (``shape'' in numpy) as $A$. Give a vectorized
expression for $\frac{\partial J}{\partial W}$ in terms of the column
vectors $\frac{\partial J}{\partial y}$ and $x$. {[}Hint: Outer
product.{]} 

\item In the usual way, define $\frac{\partial J}{\partial x}\in\reals^{d}$,
whose $i$'th entry is $\frac{\partial J}{\partial x_{i}}$. Show
that 
\[
\frac{\partial J}{\partial x}=W^{T}\left(\frac{\partial J}{\partial y}\right)
\]
{[}Note, if $x$ is just data, technically we won't need this derivative.
However, in a multilayer perceptron, $x$ may actually be the output
of a previous hidden layer, in which case we will need to propagate
the derivative through $x$ as well.{]}

\item Show that $\frac{\partial J}{\partial b}=\frac{\partial J}{\partial y}$,
where $\frac{\partial J}{\partial b}$ is defined in the usual way. 
\setcounter{saveenum}{\value{enumi}}
\end{enumerate}


\nyuparagraph{Element-wise Transformers}

Our nonlinear activation function nodes take an array (e.g. a vector,
matrix, higher-order tensor, etc), and apply the same nonlinear transformation
$\sigma:\reals\to\reals$ to every element of the array. Let's abuse
notation a bit, as is usually done in this context, and write $\sigma(A)$
for the array that results from applying $\sigma(\cdot)$ to each
element of $A$. If $\sigma$ is differentiable at $x\in\reals$,
then we'll write $\sigma'(x)$ for the derivative of $\sigma$ at
$x$, with $\sigma'(A)$ defined analogously to $\sigma(A)$.

Suppose the objective function value $J$ is written as $J=f(\sigma(A))$,
for some function $f:S\mapsto\reals$, where $S$ is an array of the
same dimensions as $\sigma(A)$ and $A$. As before, we want to find
the array $\frac{\partial J}{\partial A}$ for any $A$. Suppose for
some $A$ we have already computed the array $\frac{\partial J}{\partial S}=\frac{\partial f(S)}{\partial S}$
for $S=\sigma(A)$. At this point, we'll want to use the chain rule
to figure out $\frac{\partial J}{\partial A}$. However, because we're
dealing with arrays of arbitrary shapes, it can be tricky to write
down the chain rule. Appropriately, we'll use a tricky convention:
We'll assume all entries of an array $A$ are indexed by a single
variable. So, for example, to sum over all entries of an array $A$,
we'll just write $\sum_{i}A_{i}$. 
\begin{enumerate}
\setcounter{enumi}{\value{saveenum}}
\item Show that $\frac{\partial J}{\partial A}=\frac{\partial J}{\partial S}\odot\sigma'(A)$,
where we're using $\odot$ to represent the \textbf{Hadamard product}.
If $A$ and $B$ are arrays of the same shape, then their Hadamard
product $A\odot B$ is an array with the same shape as $A$ and $B$,
and for which $\left(A\odot B\right)_{i}=A_{i}B_{i}$. That is, it's
just the array formed by multiplying corresponding elements of $A$
and $B$. Conveniently, in \texttt{numpy} if \texttt{A} and \texttt{B}
are arrays of the same shape, then \texttt{A{*}B} is their Hadamard
product.

\setcounter{saveenum}{\value{enumi}}
\end{enumerate}

\subsection{MLP Implementation}
\begin{enumerate}
\setcounter{enumi}{\value{saveenum}}
\item Complete the class \texttt{AffineNode} in \texttt{nodes.py}. Be sure
to propagate the gradient with respect to $x$ as well, since when
we stack these layers, $x$ will itself be the output of another node
that depends on our optimization parameters. If your code is correct, you should be able to pass test\_AffineNode in \texttt{mlp\_regression.t.py}. Please attach a screenshot that shows the test results for this question. 

\item Complete the class \texttt{TanhNode} in \texttt{nodes.py}. As you'll
recall, $\frac{d}{dx}\tanh(x)=1-\tanh^{2}x$. Note that in the forward
pass, we'll already have computed $\tanh$ of the input and stored
it in self.out. So make sure to use \texttt{self.out} and not recalculate
it in the backward pass. If your code is correct, you should be able to pass test\_TanhNode in \texttt{mlp\_regression.t.py}. Please attach a screenshot that shows the test results for this question. 

\item Implement an MLP by completing the skeleton code in \texttt{mlp\_regression.py}
and making use of the nodes above. Your code should pass the tests
provided in \texttt{mlp\_regression.t.py}. Note that to break the symmetry of the problem, we initialize our weights to small random values, rather than all zeros, as we often do for convex optimization problems. Run the MLP for the two settings given in the \texttt{main()} function and report the average \textbf{training} error. Note that with an MLP, we can take the original scalar as input, in the hopes that it will learn nonlinear features on its own, using the hidden layers. In practice, it is quite challenging to get such a neural network to fit as well as one where we provide features.
\setcounter{saveenum}{\value{enumi}}
\end{enumerate}

\subsection{Multiclass classification with an MLP [Optional]}
We consider a generic classification problem with $K$ classes over inputs 
$x$ of dimension $d$. Using a MLP we will compute a K-dimensional vector $z$ representing scores, 
$$
z = W_2 \tanh (W_1 x + b_1) + b_2,
$$
with $W_1 \in \reals^{m\times d}$, $b_1 \in \reals^m$, $W_2 \in \reals^{K \times m}$ and $b_1 \in \reals^K$.
Our model assumes that $x$ belongs to class $k$ with probability $$ e^{z_k}/\sum_{k=1}^K e^{z_k},$$
which corresponds to applying a Softmax to the scores. Given this probabilistic model we can train the model by minimizing the negative log-likelihood.
\begin{enumerate}
\setcounter{enumi}{\value{saveenum}}
\item Implement a Softmax node. We provided skeleton code for class SoftmaxNode in \texttt{nodes.py}. If your code is correct, you should be able to pass test\_SoftmaxNode in \texttt{multiclass.t.py}. Please attach a screenshot that shows the test results for this question.
\item Implement a negative log-likelihood loss node for multiclass
classification. We provided skeleton code for class NLLNode in \texttt{nodes.py}. The test code for this question is combined with the test code for the next question. 
\item Implement a MLP for multiclass classification by completing the skeleton code in \texttt{multiclass.py}. Your code should pass the tests in test\_multiclass provided in multiclass.t.py. Please attach a screenshot that shows the test results for this question. 
\setcounter{saveenum}{\value{enumi}}
\end{enumerate}

\end{document}
