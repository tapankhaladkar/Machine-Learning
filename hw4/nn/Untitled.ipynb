{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d05a1f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e379842",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNode(object):\n",
    "    \"\"\"Computation graph node having no input but simply holding a value\"\"\"\n",
    "    def __init__(self, node_name):\n",
    "        self.node_name = node_name\n",
    "        self.out = None\n",
    "        self.d_out = None\n",
    "\n",
    "    def forward(self):\n",
    "        self.d_out = np.zeros(self.out.shape)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self):\n",
    "        pass\n",
    "\n",
    "    def get_predecessors(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6f0f5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorScalarAffineNode(object):\n",
    "    \"\"\" Node computing an affine function mapping a vector to a scalar.\"\"\"\n",
    "    def __init__(self, x, w, b, node_name):\n",
    "        \"\"\" \n",
    "        Parameters:\n",
    "        x: node for which x.out is a 1D numpy array\n",
    "        w: node for which w.out is a 1D numpy array of same size as x.out\n",
    "        b: node for which b.out is a numpy scalar (i.e. 0dim array)\n",
    "        node_name: node's name (a string)\n",
    "        \"\"\"\n",
    "        self.node_name = node_name\n",
    "        self.out = None\n",
    "        self.d_out = None\n",
    "        self.x = x\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "    \n",
    "    def forward(self):\n",
    "        self.out = np.dot(self.x.out, self.w.out) + self.b.out\n",
    "        self.d_out = np.zeros(self.out.shape)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self):\n",
    "        d_x = self.d_out * self.w.out\n",
    "        d_w = self.d_out * self.x.out\n",
    "        d_b = self.d_out\n",
    "        self.x.d_out += d_x\n",
    "        self.w.d_out += d_w\n",
    "        self.b.d_out += d_b\n",
    "\n",
    "    def get_predecessors(self):\n",
    "        return [self.x, self.w, self.b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d62c346a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquaredL2DistanceNode(object):\n",
    "    \"\"\" Node computing L2 distance (sum of square differences) between 2 arrays.\"\"\"\n",
    "    def __init__(self, a, b, node_name):\n",
    "        \"\"\" \n",
    "        Parameters:\n",
    "        a: node for which a.out is a numpy array\n",
    "        b: node for which b.out is a numpy array of same shape as a.out\n",
    "        node_name: node's name (a string)\n",
    "        \"\"\"\n",
    "        self.node_name = node_name\n",
    "        self.out = None\n",
    "        self.d_out = None\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        # Variable for caching values between forward and backward\n",
    "        self.a_minus_b = None\n",
    "\n",
    "    def forward(self):\n",
    "        self.a_minus_b = self.a.out - self.b.out\n",
    "        self.out = np.sum(self.a_minus_b ** 2)\n",
    "        self.d_out = np.zeros(self.out.shape)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self):\n",
    "        d_a = self.d_out * 2 * self.a_minus_b\n",
    "        d_b = -self.d_out * 2 * self.a_minus_b\n",
    "        self.a.d_out += d_a\n",
    "        self.b.d_out += d_b\n",
    "        return self.d_out\n",
    "\n",
    "    def get_predecessors(self):\n",
    "        return [self.a, self.b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "736abf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class L2NormPenaltyNode(object):\n",
    "    \"\"\" Node computing l2_reg * ||w||^2 for scalars l2_reg and vector w\"\"\"\n",
    "    def __init__(self, l2_reg, w, node_name):\n",
    "        \"\"\" \n",
    "        Parameters:\n",
    "        l2_reg: a numpy scalar array (e.g. np.array(.01)) (not a node)\n",
    "        w: a node for which w.out is a numpy vector\n",
    "        node_name: node's name (a string)\n",
    "        \"\"\"\n",
    "        self.node_name = node_name\n",
    "        self.out = None\n",
    "        self.d_out = None\n",
    "        self.l2_reg = np.array(l2_reg)\n",
    "        self.w = w\n",
    "\n",
    "    def forward(self):\n",
    "        self.out = self.l2_reg * np.sum(self.w.out ** 2)\n",
    "        self.d_out = np.zeros(self.out.shape)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self):\n",
    "        d_w = self.d_out * 2 * self.l2_reg * self.w.out\n",
    "        self.w.d_out += d_w\n",
    "\n",
    "    def get_predecessors(self):\n",
    "        return [self.w]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "266f9e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumNode(object):\n",
    "    \"\"\" Node computing a + b, for numpy arrays a and b\"\"\"\n",
    "    def __init__(self, a, b, node_name):\n",
    "        \"\"\" \n",
    "        Parameters:\n",
    "        a: node for which a.out is a numpy array\n",
    "        b: node for which b.out is a numpy array of the same shape as a\n",
    "        node_name: node's name (a string)\n",
    "        \"\"\"\n",
    "        self.node_name = node_name\n",
    "        self.out = None\n",
    "        self.d_out = None\n",
    "        self.b = b\n",
    "        self.a = a\n",
    "\n",
    "    def forward(self):\n",
    "        self.out = self.a.out + self.b.out\n",
    "        self.d_out = np.zeros(self.out.shape)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self):\n",
    "        self.a.d_out += self.d_out\n",
    "        self.b.d_out += self.d_out\n",
    "\n",
    "    def get_predecessors(self):\n",
    "        return [self.a, self.b]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffedc814",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AffineNode(object):\n",
    "    \"\"\"Node implementing affine transformation (W,x,b)-->Wx+b, where W is a matrix,\n",
    "    and x and b are vectors\n",
    "        Parameters:\n",
    "        W: node for which W.out is a numpy array of shape (m,d)\n",
    "        x: node for which x.out is a numpy array of shape (d)\n",
    "        b: node for which b.out is a numpy array of shape (m) (i.e. vector of length m)\n",
    "    \"\"\"\n",
    "    def __init__(self, W, x, b, node_name):\n",
    "        self.node_name = node_name\n",
    "        self.out = None\n",
    "        self.d_out = None\n",
    "        self.W = W  # Shape (m,d)\n",
    "        self.x = x  # Shape (d)\n",
    "        self.b = b  # Shape (m)\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Compute Wx + b where:\n",
    "        - W.out is matrix of shape (m,d)\n",
    "        - x.out is vector of shape (d)\n",
    "        - b.out is vector of shape (m)\n",
    "        Returns vector of shape (m)\n",
    "        \"\"\"\n",
    "        self.out = np.dot(self.W.out, self.x.out) + self.b.out\n",
    "        self.d_out = np.zeros(self.out.shape)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        If y = Wx + b, then:\n",
    "        dJ/dW = outer(dJ/dy, x)\n",
    "        dJ/dx = W^T * dJ/dy\n",
    "        dJ/db = dJ/dy\n",
    "        \"\"\"\n",
    "        # Gradient w.r.t W: dJ/dW = outer(dJ/dy, x)\n",
    "        d_W = np.outer(self.d_out, self.x.out)\n",
    "        self.W.d_out += d_W\n",
    "\n",
    "        # Gradient w.r.t x: dJ/dx = W^T * dJ/dy\n",
    "        d_x = np.dot(self.W.out.T, self.d_out)\n",
    "        self.x.d_out += d_x\n",
    "\n",
    "        # Gradient w.r.t b: dJ/db = dJ/dy\n",
    "        d_b = self.d_out\n",
    "        self.b.d_out += d_b\n",
    "\n",
    "    def get_predecessors(self):\n",
    "        \"\"\"Return a list of nodes that are inputs to this node\"\"\"\n",
    "        return [self.W, self.x, self.b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d4753d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TanhNode(object):\n",
    "    \"\"\"Node tanh(a), where tanh is applied elementwise to the array a\n",
    "        Parameters:\n",
    "        a: node for which a.out is a numpy array\n",
    "    \"\"\"\n",
    "    def __init__(self, a, node_name):\n",
    "        self.node_name = node_name\n",
    "        self.out = None\n",
    "        self.d_out = None\n",
    "        self.a = a\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Compute tanh(a) elementwise\n",
    "        Store result in self.out for use in backward pass\n",
    "        \"\"\"\n",
    "        self.out = np.tanh(self.a.out)\n",
    "        self.d_out = np.zeros(self.out.shape)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        If y = tanh(x), then dy/dx = 1 - tanh²(x)\n",
    "        We already have tanh(x) stored in self.out from the forward pass\n",
    "        \"\"\"\n",
    "        # d_tanh = 1 - tanh²(x)\n",
    "        d_a = self.d_out * (1 - self.out ** 2)\n",
    "        self.a.d_out += d_a\n",
    "\n",
    "    def get_predecessors(self):\n",
    "        \"\"\"Return a list of nodes that are inputs to this node\"\"\"\n",
    "        return [self.a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94437acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxNode(object):\n",
    "    \"\"\"Node implementing softmax function\n",
    "       z -> exp(z)/sum(exp(z))\n",
    "    \"\"\"\n",
    "    def __init__(self, z, node_name):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        z: node for which z.out is a numpy array\n",
    "        node_name: node's name (a string)\n",
    "        \"\"\"\n",
    "        self.node_name = node_name\n",
    "        self.z = z\n",
    "        self.out = None\n",
    "        self.d_out = None\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"Compute softmax: exp(z)/sum(exp(z))\"\"\"\n",
    "        # Subtract max for numerical stability\n",
    "        z_shifted = self.z.out - np.max(self.z.out)\n",
    "        exp_z = np.exp(z_shifted)\n",
    "        self.out = exp_z / np.sum(exp_z)\n",
    "        self.d_out = np.zeros(self.out.shape)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Compute gradient of softmax using the fact that:\n",
    "        ∂p_i/∂z_j = p_i(δ_ij - p_j)\n",
    "        where δ_ij is 1 if i=j and 0 otherwise\n",
    "        \"\"\"\n",
    "        n = self.out.shape[0]\n",
    "        # Create Jacobian matrix\n",
    "        J = np.zeros((n, n))\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                if i == j:\n",
    "                    J[i,j] = self.out[i] * (1 - self.out[i])\n",
    "                else:\n",
    "                    J[i,j] = -self.out[i] * self.out[j]\n",
    "                    \n",
    "        # Compute gradient using d_out and Jacobian\n",
    "        grad = np.dot(self.d_out, J)\n",
    "        self.z.d_out += grad\n",
    "\n",
    "    def get_predecessors(self):\n",
    "        \"\"\"Return the predecessor nodes\"\"\"\n",
    "        return [self.z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52416d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLLNode(object):\n",
    "    \"\"\" Node computing negative log likelihood loss for classification\n",
    "    Takes a vector of probabilities and a class label and returns the negative\n",
    "    log probability of that label\n",
    "    \"\"\"\n",
    "    def __init__(self, probs, y, node_name):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        probs: node for which probs.out is a numpy array of shape (K,)\n",
    "              containing probabilties of K classes (from softmax)\n",
    "        y: node containing the true class label (as an integer)\n",
    "        node_name: node's name (a string)\n",
    "        \"\"\"\n",
    "        self.node_name = node_name\n",
    "        self.probs = probs\n",
    "        self.y = y\n",
    "        self.out = None\n",
    "        self.d_out = None\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"Compute negative log likelihood loss: -log(p_y)\"\"\"\n",
    "        # Get probability of true class (adding small epsilon for numerical stability)\n",
    "        eps = 1e-15\n",
    "        prob_true_class = self.probs.out[self.y.out] + eps\n",
    "        # Compute negative log likelihood\n",
    "        self.out = -np.log(prob_true_class)\n",
    "        self.d_out = np.zeros(self.out.shape)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        If L = -log(p_y), then:\n",
    "        dL/dp_k = -1/p_y if k = y\n",
    "                  0      if k ≠ y\n",
    "        \"\"\"\n",
    "        # Initialize gradient vector\n",
    "        grad = np.zeros_like(self.probs.out)\n",
    "        # Set gradient for true class\n",
    "        grad[self.y.out] = -1.0 / (self.probs.out[self.y.out] + 1e-15)\n",
    "        # Multiply by upstream gradient\n",
    "        grad = grad * self.d_out\n",
    "        # Add to probs gradient\n",
    "        self.probs.d_out += grad\n",
    "\n",
    "    def get_predecessors(self):\n",
    "        \"\"\"Return the predecessor nodes\"\"\"\n",
    "        return [self.probs, self.y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "619fd24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class MultinomialRegressionMLP(BaseEstimator):\n",
    "    \"\"\" MLP for multiclass classification with computation graph \"\"\"\n",
    "    def __init__(self, num_hidden_units=10, num_classes=4, step_size=0.5, init_param_scale=0.01, max_num_epochs=1000):\n",
    "        self.num_hidden_units = num_hidden_units\n",
    "        self.num_classes = num_classes\n",
    "        self.init_param_scale = init_param_scale\n",
    "        self.max_num_epochs = max_num_epochs\n",
    "        self.step_size = step_size\n",
    "\n",
    "        # Initialize nodes for parameters and biases\n",
    "        self.W1 = nodes.ValueNode(node_name=\"W1\")  # First layer weights\n",
    "        self.b1 = nodes.ValueNode(node_name=\"b1\")  # First layer bias\n",
    "        self.W2 = nodes.ValueNode(node_name=\"W2\")  # Second layer weights\n",
    "        self.b2 = nodes.ValueNode(node_name=\"b2\")  # Second layer bias\n",
    "\n",
    "        # Create input and label nodes\n",
    "        self.x = nodes.ValueNode(node_name=\"x\")    # Input features\n",
    "        self.y = nodes.ValueNode(node_name=\"y\")    # True class label\n",
    "\n",
    "        # Hidden layer computation\n",
    "        self.L = nodes.AffineNode(W=self.W1, x=self.x, b=self.b1, node_name=\"L\")\n",
    "        self.h = nodes.TanhNode(a=self.L, node_name=\"h\")\n",
    "\n",
    "        # Output layer computation (scores)\n",
    "        self.scores = nodes.AffineNode(W=self.W2, x=self.h, b=self.b2, node_name=\"scores\")\n",
    "        \n",
    "        # Softmax to convert scores to probabilities\n",
    "        self.probs = nodes.SoftmaxNode(z=self.scores, node_name=\"probs\")\n",
    "        \n",
    "        # Negative log-likelihood loss\n",
    "        self.nll = nodes.NLLNode(probs=self.probs, y=self.y, node_name=\"nll\")\n",
    "\n",
    "        # Create computation graph\n",
    "        self.graph = graph.ComputationGraphFunction(\n",
    "                                            objective=self.nll,      # First positional argument\n",
    "                                            prediction=self.probs,    # Second positional argument\n",
    "                                            inputs=[self.x],         # keyword argument\n",
    "                                            outcomes=[self.y],       # keyword argument\n",
    "                                            parameters=[self.W1, self.b1, self.W2, self.b2])  # keyword argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9044fe28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
